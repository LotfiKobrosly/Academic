{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and saving keras classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, MaxPooling2D, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.losses import categorical_crossentropy\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pickle, os, cv2\n",
    "from time import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(1, strides=(1, 1), activation='relu', kernel_size=(3,3), input_shape=(20,20,1), data_format=\"channels_last\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Conv2D(150, (5, 5), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(800, activation='relu'))\n",
    "model.add(Dense(18, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"data.csv\", delimiter=\",\").astype(int)\n",
    "L = pickle.load(open(\"categories.sav\", \"rb\"))\n",
    "X, Y = dataset[:,1:], dataset[:,0]\n",
    "n, m = dataset.shape\n",
    "labels = []\n",
    "images = []\n",
    "for i in range(n):\n",
    "    l = [0]*18\n",
    "    l[Y[i]] = 1\n",
    "    labels.append(l)\n",
    "    images.append(255-X[i].reshape(20,20))\n",
    "    \n",
    "new_path = \"./sheets/Rebelo Dataset/database1/database/syn/\"\n",
    "repositories = [\"altoClef\", \"beams\", \"flat\", \"naturals\", \"notesFlags\", \"notesOpen\", \"rests1\", \"rests2\", \"sharps\", \"time\", \"trebleClef\"]\n",
    "corresponding_labels = [0,3,6,7,8,9,10,11,12,14,15,17]\n",
    "new_images = []\n",
    "new_labels = []\n",
    "for repo in repositories:\n",
    "    fresh_path = new_path + repo + \"/\"\n",
    "    for filename in os.listdir(fresh_path):\n",
    "        file = fresh_path + filename\n",
    "        img = cv2.imread(file, 0)\n",
    "        img = cv2.resize(img, (20,20))\n",
    "        new_images.append(255-img)\n",
    "        l = [0]*18\n",
    "        l[corresponding_labels[repositories.index(repo)]] = 1\n",
    "        new_labels.append(l)\n",
    "        \n",
    "images.extend(new_images)\n",
    "labels.extend(new_labels)\n",
    "n = len(images)\n",
    "\n",
    "combination = list(zip(images,labels))\n",
    "shuffle(combination)\n",
    "\n",
    "images, labels = zip(*combination)\n",
    "\n",
    "images = np.array(images)\n",
    "images = images.reshape(images.shape[0],20,20,1)\n",
    "labels = np.array(labels)\n",
    "#labels = labels.reshape((labels.shape[1], labels.shape[0]))\n",
    "#(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9414, 18)\n",
      "(9414, 20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number = n * 75 // 100\n",
    "images_train, images_test, labels_train, labels_test = images[:train_number,:,:,:], images[train_number:,:,:,:], labels[:train_number], labels[train_number:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7060, 18)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7060/7060 [==============================] - 44s 6ms/step - loss: 0.9196 - acc: 0.7429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f297657bcc0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2354/2354 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(images_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5149935523622504, 0.8453695836873407]\n"
     ]
    }
   ],
   "source": [
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"symbols_classifier.sav\"\n",
    "pickle.dump(model, open(modelname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
